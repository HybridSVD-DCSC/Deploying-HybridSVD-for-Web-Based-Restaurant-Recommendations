{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is the final implementation of the user-item matrix script. It creates 4 outputs, R_train_rest (missing last 2 interactions), R_valid_rest (the 2nd to last interaction) and R_test_rest, the last interaction. R_retrain_rest is the R matrix that contains the 2nd to last interaction but not the last (validation set added back in)\n",
    "\n",
    "Min users must be 3 or more for non-trivial results (otherwise there are users with zero training data in R_train_rest)\n"
   ],
   "id": "50c176bb4f925036"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T12:21:02.415990Z",
     "start_time": "2026-01-15T12:20:37.970739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjustable parameter for N-core filter\n",
    "MIN_REVIEWS_PER_USER = 20\n",
    "\n",
    "# Paths\n",
    "FILE_PATH = 'Data/review-District_of_Columbia.json' # Raw dataset\n",
    "# Input file remains in Output_Data (assuming previous step saved it there)\n",
    "RESTAURANT_IDS_FILE = 'Output_Data/restaurant_gmap_ids_1215.json'\n",
    "# NEW: Save outputs to n-core folder\n",
    "OUTPUT_DIR = f'{MIN_REVIEWS_PER_USER}_core_1215'\n",
    "\n",
    "# Output Filenames (Dynamic based on N)\n",
    "TRAIN_FILE = os.path.join(OUTPUT_DIR, f'R_train_rest_{MIN_REVIEWS_PER_USER}.npz')\n",
    "VALID_FILE = os.path.join(OUTPUT_DIR, f'R_valid_rest_{MIN_REVIEWS_PER_USER}.npz')\n",
    "TEST_FILE  = os.path.join(OUTPUT_DIR, f'R_test_rest_{MIN_REVIEWS_PER_USER}.npz')\n",
    "RETRAIN_FILE = os.path.join(OUTPUT_DIR, f'R_retrain_rest_{MIN_REVIEWS_PER_USER}.npz') # Train + Valid\n",
    "\n",
    "USER_MAP_FILE = os.path.join(OUTPUT_DIR, f'restaurant_user_map_rest_{MIN_REVIEWS_PER_USER}.json')\n",
    "ITEM_MAP_FILE = os.path.join(OUTPUT_DIR, f'restaurant_item_map_rest_{MIN_REVIEWS_PER_USER}.json')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def main():\n",
    "    print(f\"--- Starting N-Core Matrix Generation (N={MIN_REVIEWS_PER_USER}) ---\")\n",
    "\n",
    "    # 1. Load Valid Restaurant IDs\n",
    "    print(f\"Loading restaurant IDs from {RESTAURANT_IDS_FILE}...\")\n",
    "    try:\n",
    "        with open(RESTAURANT_IDS_FILE, 'r') as f:\n",
    "            valid_restaurant_ids = set(json.load(f))\n",
    "        print(f\"Loaded {len(valid_restaurant_ids)} valid restaurant IDs.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {RESTAURANT_IDS_FILE}. Please run the filter script first.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"Loading data from {FILE_PATH}...\")\n",
    "\n",
    "    extracted_data = {\n",
    "        'user_id': [],\n",
    "        'gmap_id': [],\n",
    "        'rating': [],\n",
    "        'time': []\n",
    "    }\n",
    "\n",
    "    # 2. Load Review Data & Filter Non-Restaurants\n",
    "    try:\n",
    "        with open(FILE_PATH, 'r') as f:\n",
    "            # Handle both list-of-dicts and line-delimited JSON\n",
    "            try:\n",
    "                first_char = f.read(1)\n",
    "                f.seek(0)\n",
    "                if first_char == '[':\n",
    "                    data = json.load(f)\n",
    "                    iterator = data\n",
    "                else:\n",
    "                    iterator = f\n",
    "            except Exception:\n",
    "                f.seek(0)\n",
    "                iterator = f\n",
    "\n",
    "            for i, record in enumerate(iterator):\n",
    "                if isinstance(record, str):\n",
    "                    try:\n",
    "                        record = json.loads(record)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "                gmap_id = record.get('gmap_id')\n",
    "\n",
    "                # FIRST FILTER: Only keep restaurants\n",
    "                if gmap_id in valid_restaurant_ids:\n",
    "                    extracted_data['user_id'].append(record.get('user_id'))\n",
    "                    extracted_data['gmap_id'].append(gmap_id)\n",
    "                    extracted_data['rating'].append(record.get('rating'))\n",
    "                    extracted_data['time'].append(record.get('time'))\n",
    "\n",
    "                if i % 100000 == 0 and i > 0:\n",
    "                    print(f\"Processed {i} raw rows...\", end='\\r')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Review file not found at {FILE_PATH}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 3. Create DataFrame\n",
    "    df = pd.DataFrame(extracted_data)\n",
    "    del extracted_data\n",
    "    print(f\"\\nInitial Restaurant Reviews: {len(df)}\")\n",
    "\n",
    "    # Clean missing data\n",
    "    df.dropna(subset=['user_id', 'gmap_id', 'rating', 'time'], inplace=True)\n",
    "\n",
    "    # 4. Apply N-Core Filter (Users must have >= N restaurant reviews)\n",
    "    print(f\"Applying {MIN_REVIEWS_PER_USER}-core filter...\")\n",
    "\n",
    "    # Count reviews per user\n",
    "    user_counts = df['user_id'].value_counts()\n",
    "\n",
    "    # Identify valid users\n",
    "    valid_users = user_counts[user_counts >= MIN_REVIEWS_PER_USER].index\n",
    "\n",
    "    # Filter DataFrame\n",
    "    initial_users = len(user_counts)\n",
    "    df = df[df['user_id'].isin(valid_users)]\n",
    "\n",
    "    final_users = df['user_id'].nunique()\n",
    "    print(f\"Users reduced from {initial_users} to {final_users}\")\n",
    "    print(f\"Total Reviews remaining: {len(df)}\")\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"Error: Dataset empty after N-core filter. Lower the N value.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 5. Map IDs to Integers\n",
    "    print(\"Mapping IDs...\")\n",
    "\n",
    "    # User Mapping\n",
    "    # Users are mapped based on who remains in the N-core set\n",
    "    user_cat = df['user_id'].astype('category')\n",
    "    user_indices = user_cat.cat.codes\n",
    "    user_mapper = dict(enumerate(user_cat.cat.categories))\n",
    "\n",
    "    # Item Mapping (CRITICAL UPDATE)\n",
    "    # We must include ALL valid restaurants in the mapping, even those with 0 reviews in this subset.\n",
    "    # We sort them to ensure a deterministic order.\n",
    "    all_restaurants_list = sorted(list(valid_restaurant_ids))\n",
    "\n",
    "    # Force the column to be categorical with the full set of categories\n",
    "    df['gmap_id'] = pd.Categorical(df['gmap_id'], categories=all_restaurants_list)\n",
    "\n",
    "    # Get codes (this works because we set the categories explicitly above)\n",
    "    item_indices = df['gmap_id'].cat.codes\n",
    "\n",
    "    # Create the item mapper for the full set\n",
    "    item_mapper = {i: v for i, v in enumerate(all_restaurants_list)}\n",
    "\n",
    "    df['user_idx'] = user_indices\n",
    "    df['item_idx'] = item_indices\n",
    "\n",
    "    num_users = len(user_mapper)\n",
    "    num_items = len(item_mapper) # This now equals len(valid_restaurant_ids)\n",
    "\n",
    "    print(f\"Matrix Dimensions: {num_users} Users x {num_items} Items\")\n",
    "    print(\"(Note: num_items matches the total number of restaurants, preserving empty columns)\")\n",
    "\n",
    "    # 6. Split Data (Train / Valid / Test)\n",
    "    print(\"Splitting data (Test=Last, Valid=2nd Last)...\")\n",
    "\n",
    "    # Sort by User then Time (Newest first)\n",
    "    df.sort_values(by=['user_idx', 'time'], ascending=[True, False], inplace=True)\n",
    "\n",
    "    # Calculate rank: 0 = Newest, 1 = 2nd Newest, etc.\n",
    "    df['rank'] = df.groupby('user_idx').cumcount()\n",
    "\n",
    "    df_test = df[df['rank'] == 0]\n",
    "    df_valid = df[df['rank'] == 1]\n",
    "    df_train = df[df['rank'] >= 2]\n",
    "\n",
    "    print(f\"Train size: {len(df_train)}\")\n",
    "    print(f\"Valid size: {len(df_valid)}\")\n",
    "    print(f\"Test size:  {len(df_test)}\")\n",
    "\n",
    "    # 7. Build Sparse Matrices\n",
    "    print(\"Constructing Sparse Matrices...\")\n",
    "    shape = (num_users, num_items)\n",
    "\n",
    "    R_train = csr_matrix((df_train['rating'], (df_train['user_idx'], df_train['item_idx'])), shape=shape)\n",
    "    R_valid = csr_matrix((df_valid['rating'], (df_valid['user_idx'], df_valid['item_idx'])), shape=shape)\n",
    "    R_test  = csr_matrix((df_test['rating'],  (df_test['user_idx'],  df_test['item_idx'])),  shape=shape)\n",
    "\n",
    "    # Create Retrain Matrix (Train + Valid)\n",
    "    print(\"Creating Retrain Matrix (Train + Valid)...\")\n",
    "    R_retrain = R_train + R_valid\n",
    "\n",
    "    # 8. Save\n",
    "    print(f\"Saving outputs to '{OUTPUT_DIR}'...\")\n",
    "    save_npz(TRAIN_FILE, R_train)\n",
    "    save_npz(VALID_FILE, R_valid)\n",
    "    save_npz(TEST_FILE, R_test)\n",
    "    save_npz(RETRAIN_FILE, R_retrain)\n",
    "\n",
    "    with open(USER_MAP_FILE, 'w') as f:\n",
    "        json.dump({str(k): str(v) for k, v in user_mapper.items()}, f)\n",
    "    with open(ITEM_MAP_FILE, 'w') as f:\n",
    "        json.dump({str(k): str(v) for k, v in item_mapper.items()}, f)\n",
    "\n",
    "    print(\"Done! Files generated:\")\n",
    "    print(f\"1. {TRAIN_FILE}\")\n",
    "    print(f\"2. {VALID_FILE}\")\n",
    "    print(f\"3. {TEST_FILE}\")\n",
    "    print(f\"4. {RETRAIN_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "af67f14ec2feb828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting N-Core Matrix Generation (N=20) ---\n",
      "Loading restaurant IDs from Output_Data/restaurant_gmap_ids_1215.json...\n",
      "Loaded 3484 valid restaurant IDs.\n",
      "Loading data from Data/review-District_of_Columbia.json...\n",
      "Processed 1800000 raw rows...\r\n",
      "Initial Restaurant Reviews: 962174\n",
      "Applying 20-core filter...\n",
      "Users reduced from 423234 to 4060\n",
      "Total Reviews remaining: 147160\n",
      "Mapping IDs...\n",
      "Matrix Dimensions: 4060 Users x 3484 Items\n",
      "(Note: num_items matches the total number of restaurants, preserving empty columns)\n",
      "Splitting data (Test=Last, Valid=2nd Last)...\n",
      "Train size: 139040\n",
      "Valid size: 4060\n",
      "Test size:  4060\n",
      "Constructing Sparse Matrices...\n",
      "Creating Retrain Matrix (Train + Valid)...\n",
      "Saving outputs to '20_core_1215'...\n",
      "Done! Files generated:\n",
      "1. 20_core_1215\\R_train_rest_20.npz\n",
      "2. 20_core_1215\\R_valid_rest_20.npz\n",
      "3. 20_core_1215\\R_test_rest_20.npz\n",
      "4. 20_core_1215\\R_retrain_rest_20.npz\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1daae1f651fa210f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
